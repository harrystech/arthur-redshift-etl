{
    # General settings controlling how Arthur itself behaves
    "arthur_settings": {
        # If an extract from an upstream source or copy from S3 files fails due to some transient error, retry the extract at most this many times. Zero disables retries
        "extract_retries": 3, 
        "copy_data_retries": 3,
        # If normal approach to refreshing a materialized relation (CTAS or source) fails, fill the relation with
        # data already in the warehouse from a prior build of that relation.
        # Setting determines when Arthur should fallback to prior data:
        #   "designed" - use (instead of query) only on relations whose design file explicitly sets "loads_from_prior_data"
        #   "source" - use for source relations where load has failed or concurrent extract failed or timed out
        #   "non_key" - use for any failed materialized relation other than CTAS with non-"identity" columns named '*_key'
        #   "all" - use for any failed materialized relation
        "loads_from_prior_data": "source"
    },
    # Target (Redshift) cluster
    "data_warehouse": {
        # The environment variable must contain a full connection string for an admin user to create a database.
        "admin_access": "DATA_WAREHOUSE_ADMIN",
        # The environment variable must contain a full connection string for an ETL user.
        "etl_access": "DATA_WAREHOUSE_ETL",
        # All schemas, tables, etc. will be assigned to this user.  The owner's group will be the ETL group.
        "owner": {
            "name": "dw",
            "group": "etl_rw"

        },
        "users": [
            {
                # Default group specified as group of pseudo-user "default"
                "name": "default",
                "group": "analyst_ro"
            }
        ]
    },
    # Logging of the ETL into events tables
    "etl_events": {
        "enabled": True,  # required to be enabled for concurrent extract/load
        "read_capacity": 3,
        "write_capacity": 3
    },
    # Type information from source (PostgreSQL) to target (Redshift)
    "type_maps": {
        # Types that may be used "as-is", see also
        # http://docs.aws.amazon.com/redshift/latest/dg/c_Supported_data_types.html
        # The keys are regular expression (with escaped backslashes!) and the
        # values are the serialization formats in Avro files.
        "as_is_att_type": {
            "smallint": "int",
            "integer": "int",
            "bigint": "long",
            "real": "double",
            "double precision": "double",
            "numeric\\(\\d+,\\d+\\)": "string",
            "boolean": "boolean",
            "character\\(\\d+\\)": "string",
            "character varying\\(\\d+\\)": "string",
            "date": "string",
            "timestamp without time zone": "string"
        },
        # Map of known PostgreSQL attribute types to usable types in Redshift.  Missing types will cause an exception
        # The first element in the list is the new type, the second element is the necessary cast expression,
        # the third element is the serialization format in Avro files.
        # Note that in every expression, %s is replaced by the column name within quotes.
        "cast_needed_att_type": {
            "int4range": ["varchar(65535)", "%s::varchar(65535)", "string"],
            "integer\\[\\]": ["varchar(65535)", "%s::varchar(65535)", "string"],
            "bigint\\[\\]": ["varchar(65535)", "%s::varchar(65535)", "string"],
            # NOTE varchar counts characters but Redshift is byte limitations.
            # We are setting the max here to 10k ... you need to evaluate on live table what that the limit should be.
            "character varying": ["varchar(10000)", "%s::varchar(10000)", "string"],
            "text": ["varchar(10000)", "%s::varchar(10000)", "string"],
            "time without time zone": ["varchar(256)", "%s::varchar(256)", "string"],
            # CAVEAT EMPTOR This only works if your database is running in UTC.
            "timestamp with time zone": ["timestamp without time zone", "%s::varchar(256)", "string"],
            "json": ["varchar(65535)", "%s::varchar(65535)", "string"],
            # N.B. This requires a PostgreSQL version of 9.3 or better.
            "hstore": ["varchar(65535)", "public.hstore_to_json(%s)::varchar(65535)", "string"],
            "uuid": ["varchar(36)", "%s::varchar(36)", "string"],
            # The numeric data type without precision and scale should not be used upstream!
            "numeric": ["decimal(18,4)", "%s::decimal(18,4)", "string"],
            # The bytea data type is probably not useful, but we'll try to pull it in base64 format.
            "bytea": ["varchar(65535)", "encode(%s, 'base64')", "string"]
        }
    }
}
